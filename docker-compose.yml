version: "3.9"

services:
  dataops_postgres:
    image: postgres:15
    container_name: dataops_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - dataops_data:/var/lib/postgresql/data
      - ./scripts/init-postgres.sql:/docker-entrypoint-initdb.d/init-postgres.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 5s
      timeout: 5s
      retries: 10

  dataops_dbt:
    image: ghcr.io/dbt-labs/dbt-postgres:1.7.4
    container_name: dataops_dbt
    depends_on:
      - dataops_postgres
    environment:
      DBT_PROFILES_DIR: /usr/app/dbt
    volumes:
      - ./datasets/nyc_taxi/dbt/nyc_taxi:/usr/app/dbt
      - ./datasets/nyc_taxi/dbt/profiles.yml:/usr/app/dbt/profiles.yml:ro
    working_dir: /usr/app/dbt

  dataops_airflow_webserver:
    build:
      context: .
      dockerfile: ./airflow/Dockerfile
    container_name: dataops_airflow_webserver
    depends_on:
      dataops_postgres:
        condition: service_healthy
      dataops_dbt:
        condition: service_started
    environment:
      AIRFLOW__CORE__EXECUTOR: "SequentialExecutor"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${DB_URL}
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/src/pipeline/dags
      PYTHONPATH: /opt/airflow/src
      DB_HOST: ${DB_HOST}
    volumes:
      - ./src:/opt/airflow/src
      - ./datasets:/opt/airflow/datasets
      - ./scripts/airflow-entrypoint.sh:/scripts/airflow-entrypoint.sh:ro
    ports:
      - "8080:8080"
    entrypoint: ["/bin/bash", "/scripts/airflow-entrypoint.sh"]
    command: webserver

  dataops_airflow_scheduler:
    build:
      context: .
      dockerfile: ./airflow/Dockerfile
    container_name: dataops_airflow_scheduler
    depends_on:
      dataops_postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: "SequentialExecutor"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${DB_URL}
      AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/src/pipeline/dags
      PYTHONPATH: /opt/airflow/src
      DB_HOST: ${DB_HOST}
    volumes:
      - ./src:/opt/airflow/src
      - ./datasets:/opt/airflow/datasets
    command: scheduler

volumes:
  dataops_data:
